\chapter{Konzept}
In diesem Kapitel wird das Konzept für die Messkriterien und die zu verwendenden Beispielanfragen beschrieben.

\section{Vorgehen}
Um dem Nutzer einen echten Mehrwert einer weiteren Bibelsuche anbieten zu können, müssen existierende Suchmaschinen analysiert und bewertet werden.
Dazu sollen \textit{Kriterien für die Messbarkeit} einer Suchmaschine und deren Resultaten definiert werden, so dass verschiedene Suchmaschinen miteinander verglichen werden können.
Weiter werden Beispielanfragen definiert, die in allen Suchmaschinen ausgeführt werden und so die Basis für den Vergleich mit der neuen Suche bilden.

\section{Kriterien für die Messbarkeit}
%Effizienz (Ressourcen Verbrauch)
%Effektivität (Korrektheit)
%Precision = (Relevant & Gefunden) / Gefunden
%	—> Genauigkeit
% Precision: TP / (TP+FP)
%Recall = (Relevant & Gefunden) / Relevant
%	—> Vollständigkeit
% Recall: TP/Posives = TP / (TP+FN)
% Benchmarking
% ressourcen verbrauch
% Mean average precision: Average precision
Um die Qualität einer Suchmaschine zu messen gibt es verschiedene Kennzahlen, die aus dem Resultat von Beispielanfragen abgeleitet werden können.
Diese Kennzahlen erlauben dann einen Vergleich von verschiedenen Suchmaschinen.

Die \textit{Precision} bezeichnet die \textbf{Genauigkeit} der Resultate, indem der Anteil von irrelevanten Ergebnissen gemessen wird.
Somit bedeutet $precision = 0$, dass kein einziges Ergebnis relevant ist.
Werden hingegen nur korrekte Ergebnisse angezeigt, so gilt $precision = 1$.

Der \textit{Recall} misst die \textbf{Vollständigkeit} der Ergebnisse. Hier bedeutet $recall = 1$, dass alle relevanten Ergebnisse vorhanden sind, während $recall = 0$ heisst, dass keines der Ergebnisse relevant ist.

\begin{align}
	precision & = \frac{\{relevant \, documents\} \cap \{retrieved \, documents\}}{\{retrieved \, documents\}} \\
	recall & = \frac{\{relevant \, documents\} \cap \{retrieved \, documents\}}{\{relevant \, documents\}}
\end{align}
Dabei bezeichnet $\{relevant \, documents\}$ alle Dokumente, die gefunden werden sollten und $\{retrieved \, documents\}$ alle, die gefunden worden sind, inklusive denen, die gar nicht von Interesse sind.
Mit $\{relevant \, documents\} \cap \{retrieved \, documents\}$ sind folglich alle gefundenen und gleichzeitig relevanten \glspl{glos:documentLabel} beschrieben.

\textit{Precision} und \textit{Recall} sind insofern abhängig voneinander, weil oft eine Verbesserung des einen gleichzeitig eine Verschlechterung des anderen Wertes mit sich bringt.

Ein extremes Beispiel dafür ist eine Liste mit allen \glspl{glos:documentLabel}n. Der\textit{Recall} wird Eins sein, jedoch die \textit{Precision} wird gegen Null tendieren.

Das Hauptproblem dieser beiden Kennzahlen ist, dass die Anzahl relevanter \glspl{glos:documentLabel} oft nicht bekannt ist. 
Durch geschicktes Auslesen der Beispielanfragen, kann die Anzahl der relevanten \glspl{glos:documentLabel} durch menschliches Wissen abgeschätzt werden.

%Weitere wichtige Kennzahlen sind \textit{Precision at K}, \textit{R-Precision} und \textit{Mean average precision}, die auf Wikipedia genau beschrieben sind.\footcite{Information_retrieval_Wikipedia_the_free_encyclopedia_2016-05-11}


\section{Suchanfrage}

\subsection{Anfrage Typen}
Die häufigsten Suchanfragen an eine Bibel lassen sich in drei Arten unterteilen:
\begin{itemize}[noitemsep]
	\item \textbf{Faktenorientierte Anfragen}\\
		Bei der faktenorientierten Anfrage steht der in der Bibel beschriebene Sachverhalt im Vordergrund. Der Nutzer will möglich rasch eine Antwort auf seine Frage "`Wie ist es in der Bibel beschrieben?"' finden.
		Die Antwort auf eine solche Frage kann mit einem Vers beantwortet werden.
		
	\item \textbf{Kontextorientierte Anfrage}\\
		Bei der kontextorientierten Anfrage sollen möglichst viele Stellen aufgelistet werden, in denen das Thema des Suchbegriffes vorkommt. Der Nutzer hat dann verschiedene Anhaltspunkte um dort sein Wissen zu vertiefen.
		Diese Suche ist besonders nützlich, da je nach Übersetzung und Buch mit verschiedenen Worten etwas ähnliches beschrieben oder ausgedrückt wird.
		
		Im Unterschied zur faktenorientierten Anfrage ist hier die Fragestellung und Antwort nicht gleich diskret. Kontextorientierte Fragen sind zum Beispiel: "`Wo steht etwas über das Abendmahl?"' Oder: "`Wo und wie oft kehrte das Volk Israel wieder zu Gott zurück?"'
		
	\item \textbf{Versorientierte Anfragen}\\
		Die versorientierte Anfrage soll dem Nutzer helfen, einzelne Verse zu gewünschten Themen zu finden.
		Zudem lässt sich so auch der vollständige Vers inkl. Stellenangabe finden, wenn nur noch der ungefähren Inhalt bekannt ist.
		
\end{itemize}

%\todo{Abgrenzung zu Fakteninformation und problemorientierten Informationsbedarf}
%https://de.wikipedia.org/wiki/Information_Retrieval
% Der Informationsbedarf ist der Bedarf an handlungsrelevantem Wissen und kann dabei konkret und problemorientiert sein. Beim konkreten Informationsbedarf wird eine Fakteninformation benötigt. Also beispielsweise "Was ist die Hauptstadt von Frankreich?". Die Antwort "Paris" deckt den Informationsbedarf vollständig. Anders ist es beim problemorientierten Informationsbedarf. Hier werden mehrere Dokumente benötigt, um den Bedarf zu stillen. Zudem wird der problemorientierte Informationsbedarf nie ganz gedeckt werden können. Gegebenenfalls ergibt sich aus der erhaltenen Information sogar ein neuer Bedarf oder die Modifikation des ursprünglichen Bedarfs. Beim Informationsbedarf wird vom Nutzer abstrahiert. Das heißt, es wird der objektive Sachverhalt betrachtet.

